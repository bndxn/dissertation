{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21st June 2022 \n",
    "* Have found a potential task and method, but struggling to get predictions in the right size\n",
    "* Need to learn model dimensions, and why CNNs etc are not giving the right shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Hands-on, Ch10\n",
    "#### Example network, FashionMNIST NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this do?\n",
    "* Takes 28x28 image\n",
    "* Flatten produces 784 = 28*28 image\n",
    "* Dense layer has 300 nodes, and each take 784 inputs plus a bias term, so 784*300 + 300 = 235,500 parameters\n",
    "* Next layer takes output of previous 300 nodes, plus 100 bias terms, 100*300 + 100 = 30,100\n",
    "* Last layer takes output of previous 100 nodes, plus 10 bias terms, 100*10 + 10 = 1,010\n",
    "* Softmax uses to get which output has highest probability (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loss function for categories\n",
    "* Optimizer is stochastic gradient descent\n",
    "* Will optimise loss function, but also will report on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More complex models using functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We create an input_ based on the shape of the training data\n",
    "* We create the next layer as a function called hidden1, then we define hidden2 by passing hidden1 as an argument\n",
    "* Using the previous layer as an object is why this is the functional approach\n",
    "* Concat is passed as arguments both hidden2 and input_, so input_ can go directly to concat\n",
    "* Only hidden1 and hidden2 have activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "# Split the data into the first 5 features, and all the features after the second\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "\n",
    "# Some examples from the test set \n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "# Fit on the tuples A and B, with y as the dependent variable\n",
    "history = model.fit((X_train_A, X_train_B), y_train, \n",
    "                    epochs=20, \n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "* Lower hidden layers model low-level structures, e.g. line segments, intermediate ones combine them, e.g. shapes, and high-level ones model structures\n",
    "* More layers helps DNNs converge faster and helps them generalise to new datasets\n",
    "* Neurons per layer - used to decrease in pyramid shape as you go down layers, but now kept fairly constant\n",
    "* In general you do better to increase layers than neurons per layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on hands-on, ch11\n",
    "\n",
    "* Vanishing/exploding gradients is because different layers can learn at different speeds, so all the updates don't get applied if a higher layer learns quickly\n",
    "* 2015 paper suggests batch normalization as a way to deal with gradient problems, which zero-centres and scales all inputs then scales and shifts the result\n",
    "* BN adds four parameters per input \n",
    "\n",
    "### Notes on hands-on, ch14\n",
    "\n",
    "* Each filter in a CNN is a different convolutional filter, so filter=32 means 32 different filters are passed over\n",
    "* An example of a filter is one that picks out horizontal or vertical lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pooling layers shrink the input image which reduces the computational load and the number of parameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
